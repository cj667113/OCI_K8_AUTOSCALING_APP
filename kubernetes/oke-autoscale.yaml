# =========================
# AUTOSCALE APP (deployment + svc + hpa)
# =========================
apiVersion: v1
kind: Namespace
metadata:
  name: autoscale
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: autoscale-probe
  namespace: autoscale
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: autoscale-probe
rules:
- apiGroups: [""]
  resources: ["nodes", "pods"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: autoscale-probe
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: autoscale-probe
subjects:
- kind: ServiceAccount
  name: autoscale-probe
  namespace: autoscale
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: autoscale-probe
  namespace: autoscale
  labels:
    app: autoscale-probe
spec:
  replicas: 1
  selector:
    matchLabels:
      app: autoscale-probe
  template:
    metadata:
      labels:
        app: autoscale-probe
    spec:
      terminationGracePeriodSeconds: 5
      serviceAccountName: autoscale-probe
      containers:
      - name: app
        image: ghcr.io/cj667113/oci_k8_autoscaling_app:latest
        imagePullPolicy: Always
        env:
        - { name: MAX_INFLIGHT,        value: "64" }
        - { name: ACQUIRE_TIMEOUT_MS,  value: "0" }
        - { name: CPU_MS,              value: "10000" }
        - { name: PBKDF2_ITERS,        value: "200000" }
        - { name: WATCH_NAMESPACE,     value: "autoscale" }
        - { name: WATCH_SELECTOR,      value: "app=workload" }
        ports:
        - name: http
          containerPort: 8080
        readinessProbe:
          httpGet: { path: /healthz, port: http }
          initialDelaySeconds: 5
          periodSeconds: 5
        livenessProbe:
          httpGet: { path: /healthz, port: http }
          initialDelaySeconds: 10
          periodSeconds: 10
        resources:
          requests: { cpu: "200m", memory: "128Mi" }
          # Intentionally no limits → can consume full node CPU
---
apiVersion: v1
kind: Service
metadata:
  name: autoscale-probe-lb
  namespace: autoscale
  labels: { app: autoscale-probe }
  annotations:
    oci.oraclecloud.com/load-balancer-type: "nlb"
    oci.oraclecloud.com/nlb-subnet1: "ocid1.subnet.oc1.iad.aaaaaaaapluertttyvs4bgg4pfng7fwouebyn27uaexcnq4hjcqdetbh64da"
    oci.oraclecloud.com/load-balancer-internal: "false"
    oci.oraclecloud.com/load-balancer-shape: "flexible"
spec:
  type: LoadBalancer
  selector: { app: autoscale-probe }
  externalTrafficPolicy: Cluster
  ports:
  - name: http
    port: 80
    targetPort: http
---
apiVersion: v1
kind: Service
metadata:
  name: autoscale-probe-metrics
  namespace: autoscale
  labels: { app: autoscale-probe }
spec:
  type: ClusterIP
  selector: { app: autoscale-probe }
  ports:
  - name: metrics
    port: 9100
    targetPort: http
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: autoscale-probe-hpa
  namespace: autoscale
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: autoscale-probe
  minReplicas: 1
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15

# =========================
# METRICS SERVER (embedded, OKE-friendly & fixed /tmp)
# =========================
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:aggregated-metrics-reader
rules:
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods","nodes"]
  verbs: ["get","list","watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:metrics-server
rules:
- apiGroups: [""]
  resources: ["pods","nodes","nodes/stats","nodes/proxy","nodes/metrics","namespaces","configmaps"]
  verbs: ["get","list","watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: metrics-server-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: metrics-server:system:auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: v1
kind: Service
metadata:
  name: metrics-server
  namespace: kube-system
  labels: { k8s-app: metrics-server }
spec:
  selector: { k8s-app: metrics-server }
  ports:
  - name: https
    port: 443
    targetPort: https
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
  labels: { k8s-app: metrics-server }
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels: { k8s-app: metrics-server }
  template:
    metadata:
      labels: { k8s-app: metrics-server }
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: metrics-server
      tolerations:
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
      volumes:
      - name: tmp-dir
        emptyDir: {}
      containers:
      - name: metrics-server
        image: registry.k8s.io/metrics-server/metrics-server:v0.7.2
        imagePullPolicy: IfNotPresent
        args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP
        - --kubelet-use-node-status-port
        - --kubelet-insecure-tls
        - --metric-resolution=15s
        ports:
        - name: https
          containerPort: 4443
        volumeMounts:
        - name: tmp-dir
          mountPath: /tmp
        livenessProbe:
          httpGet: { path: /livez, port: https, scheme: HTTPS }
          initialDelaySeconds: 20
          periodSeconds: 10
        readinessProbe:
          httpGet: { path: /readyz, port: https, scheme: HTTPS }
          initialDelaySeconds: 20
          periodSeconds: 10
        securityContext:
          readOnlyRootFilesystem: true
        resources:
          requests: { cpu: 50m, memory: 60Mi }
          limits:   { cpu: 200m, memory: 200Mi }
---
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: v1beta1.metrics.k8s.io
spec:
  group: metrics.k8s.io
  version: v1beta1
  insecureSkipTLSVerify: true
  groupPriorityMinimum: 100
  versionPriority: 100
  service:
    name: metrics-server
    namespace: kube-system
    port: 443

# =========================
# MONITORING: Prometheus (scrapes autoscale app)
# =========================
---
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources: ["nodes","nodes/metrics","services","endpoints","pods","namespaces"]
  verbs: ["get","list","watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: monitoring
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    scrape_configs:
    - job_name: 'autoscale-probe'
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names: ['autoscale']
      relabel_configs:
      - { action: keep, source_labels: [__meta_kubernetes_service_label_app], regex: autoscale-probe }
      - { action: keep, source_labels: [__meta_kubernetes_endpoint_port_name], regex: (metrics|http) }
      - { action: labelmap, regex: __meta_kubernetes_service_label_(.*) }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
  labels: { app: prometheus }
spec:
  replicas: 1
  selector:
    matchLabels: { app: prometheus }
  template:
    metadata:
      labels: { app: prometheus }
    spec:
      serviceAccountName: prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:v2.54.1
        args:
        - --config.file=/etc/prometheus/prometheus.yml
        - --storage.tsdb.path=/prometheus
        - --web.enable-lifecycle
        ports:
        - name: http
          containerPort: 9090
        volumeMounts:
        - { name: config, mountPath: /etc/prometheus }
        - { name: data,   mountPath: /prometheus }
      volumes:
      - name: config
        configMap: { name: prometheus-config }
      - name: data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
  labels: { app: prometheus }
spec:
  type: ClusterIP
  selector: { app: prometheus }
  ports:
  - name: http
    port: 9090
    targetPort: http

# =========================
# MONITORING: Grafana (LB on :3000 — no conflict with app :80)
# =========================
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasource
  namespace: monitoring
  labels:
    grafana_datasource: "1"
data:
  prometheus.yaml: |
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus.monitoring.svc.cluster.local:9090
      isDefault: true
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  oke-autoscale.json: |
    {
      "title": "OKE Autoscale: 503s & Readiness",
      "timezone": "browser",
      "schemaVersion": 38,
      "version": 1,
      "refresh": "10s",
      "panels": [
        { "type": "timeseries", "title": "503 errors / sec",
          "targets": [{ "expr": "sum(rate(loadgen_requests_total{status=\"503\"}[1m]))" }],
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 } },
        { "type": "timeseries", "title": "Successful RPS",
          "targets": [{ "expr": "sum(rate(loadgen_requests_total{status=\"200\"}[1m]))" }],
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 } },
        { "type": "timeseries", "title": "Node creation → Ready (seconds)",
          "targets": [{ "expr": "k8s_node_ready_seconds" }],
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 } },
        { "type": "state-timeline", "title": "Node Ready state",
          "targets": [{ "expr": "k8s_node_ready_gauge" }],
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 } },
        { "type": "table", "title": "Pod Pending → Running (seconds)",
          "targets": [{ "expr": "k8s_pod_schedule_seconds" }],
          "options": { "showHeader": true },
          "gridPos": { "h": 8, "w": 24, "x": 0, "y": 16 } }
      ]
    }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dash-provider
  namespace: monitoring
data:
  dashboards.yaml: |
    apiVersion: 1
    providers:
    - name: 'default'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: false
      editable: true
      options:
        path: /var/lib/grafana/dashboards
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
  labels: { app: grafana }
spec:
  replicas: 1
  selector:
    matchLabels: { app: grafana }
  template:
    metadata:
      labels: { app: grafana }
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:10.4.5
        env:
        - { name: GF_SECURITY_ADMIN_USER, value: "admin" }
        - { name: GF_SECURITY_ADMIN_PASSWORD, value: "admin" }   # CHANGE ME
        - { name: GF_PATHS_PROVISIONING, value: /etc/grafana/provisioning }
        ports:
        - name: http
          containerPort: 3000
        volumeMounts:
        - { name: datasource,      mountPath: /etc/grafana/provisioning/datasources }
        - { name: dashboards,      mountPath: /etc/grafana/provisioning/dashboards }
        - { name: dashboards-json, mountPath: /var/lib/grafana/dashboards }
      volumes:
      - name: datasource
        configMap: { name: grafana-datasource }
      - name: dashboards
        projected:
          sources:
          - configMap: { name: grafana-dash-provider }
      - name: dashboards-json
        configMap: { name: grafana-dashboard }
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
  labels: { app: grafana }
  annotations:
    oci.oraclecloud.com/load-balancer-type: "nlb"
    oci.oraclecloud.com/nlb-subnet1: "ocid1.subnet.oc1.iad.aaaaaaaapluertttyvs4bgg4pfng7fwouebyn27uaexcnq4hjcqdetbh64da"
    oci.oraclecloud.com/load-balancer-internal: "false"
    oci.oraclecloud.com/load-balancer-shape: "flexible"
spec:
  type: LoadBalancer
  selector: { app: grafana }
  externalTrafficPolicy: Cluster
  ports:
  - name: http
    port: 80
    targetPort: http
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
rules:
- apiGroups: [""]
  resources: ["events", "endpoints", "pods/eviction", "pods/status"]
  verbs: ["create", "patch", "update", "get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods", "services", "replicationcontrollers", "persistentvolumeclaims", "persistentvolumes"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["replicasets", "statefulsets", "daemonsets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch"]
  resources: ["jobs", "cronjobs"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities", volumeattachments]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch", "update"]
- apiGroups: [""]
  resources: ["nodes/status"]
  verbs: ["patch"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["get", "list", "watch", "create", "update", "delete"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["namespaces"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
- kind: ServiceAccount
  name: cluster-autoscaler
  namespace: kube-system