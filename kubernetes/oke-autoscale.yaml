# =========================
# AUTOSCALE APP (deployment + svc + hpa)
# =========================
apiVersion: v1
kind: Namespace
metadata:
  name: autoscale
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: autoscale-probe
  namespace: autoscale
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: autoscale-probe
rules:
- apiGroups: [""]
  resources: ["nodes", "pods"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: autoscale-probe
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: autoscale-probe
subjects:
- kind: ServiceAccount
  name: autoscale-probe
  namespace: autoscale
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: autoscale-probe
  namespace: autoscale
  labels:
    app: autoscale-probe
spec:
  replicas: 1
  selector:
    matchLabels:
      app: autoscale-probe
  template:
    metadata:
      labels:
        app: autoscale-probe
    spec:
      terminationGracePeriodSeconds: 5
      serviceAccountName: autoscale-probe
      containers:
      - name: app
        image: ghcr.io/cj667113/oci_k8_autoscaling_app:latest
        imagePullPolicy: Always
        env:
        - { name: MAX_INFLIGHT,        value: "64" }
        - { name: ACQUIRE_TIMEOUT_MS,  value: "0" }
        - { name: CPU_MS,              value: "10000" }
        - { name: PBKDF2_ITERS,        value: "200000" }
        - { name: WATCH_NAMESPACE,     value: "autoscale" }
        - { name: WATCH_SELECTOR,      value: "app=workload" }
        ports:
        - name: http
          containerPort: 8080
        readinessProbe:
          httpGet: { path: /healthz, port: http }
          initialDelaySeconds: 5
          periodSeconds: 5
        livenessProbe:
          httpGet: { path: /healthz, port: http }
          initialDelaySeconds: 10
          periodSeconds: 10
        resources:
          requests:
            cpu: "1"
            memory: "1Gi"
---
apiVersion: v1
kind: Service
metadata:
  name: autoscale-probe-lb
  namespace: autoscale
  labels: { app: autoscale-probe }
  annotations:
    oci.oraclecloud.com/load-balancer-type: "lb"
    oci.oraclecloud.com/load-balancer-internal: "false"
    oci.oraclecloud.com/load-balancer-shape: "flexible"
    oci.oraclecloud.com/load-balancer-shape-flex-min: "10"   # Mbps
    oci.oraclecloud.com/load-balancer-shape-flex-max: "10"   # Mbps
    oci-load-balancer.oraclecloud.com/health-check: '{"protocol":"TCP","port":8080,"retries":2,"timeoutInMillis":5000}'
    service.beta.kubernetes.io/oci-load-balancer-security-list-management-mode: "None" # Needed for Use Pod IPs
spec:
  type: LoadBalancer
  allocateLoadBalancerNodePorts: false # Use Pod IPs
  externalTrafficPolicy: Cluster
  selector:
    app: autoscale-probe
  ports:
  - name: http
    port: 80
    targetPort: http          # -> container port 8080
---
apiVersion: v1
kind: Service
metadata:
  name: autoscale-probe-metrics
  namespace: autoscale
  labels: { app: autoscale-probe }
spec:
  type: ClusterIP
  selector: { app: autoscale-probe }
  ports:
  - name: metrics
    port: 8080
    targetPort: http
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: autoscale-probe-hpa
  namespace: autoscale
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: autoscale-probe
  minReplicas: 1
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15

# =========================
# METRICS SERVER (embedded, OKE-friendly & fixed /tmp)
# =========================
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:aggregated-metrics-reader
rules:
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods","nodes"]
  verbs: ["get","list","watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:metrics-server
rules:
- apiGroups: [""]
  resources: ["pods","nodes","nodes/stats","nodes/proxy","nodes/metrics","namespaces","configmaps"]
  verbs: ["get","list","watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: metrics-server-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: metrics-server:system:auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: v1
kind: Service
metadata:
  name: metrics-server
  namespace: kube-system
  labels: { k8s-app: metrics-server }
spec:
  selector: { k8s-app: metrics-server }
  ports:
  - name: https
    port: 443
    targetPort: https
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
  labels: { k8s-app: metrics-server }
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels: { k8s-app: metrics-server }
  template:
    metadata:
      labels: { k8s-app: metrics-server }
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: metrics-server
      tolerations:
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
      volumes:
      - name: tmp-dir
        emptyDir: {}
      containers:
      - name: metrics-server
        image: registry.k8s.io/metrics-server/metrics-server:v0.7.2
        imagePullPolicy: IfNotPresent
        args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP
        - --kubelet-use-node-status-port
        - --kubelet-insecure-tls
        - --metric-resolution=15s
        ports:
        - name: https
          containerPort: 4443
        volumeMounts:
        - name: tmp-dir
          mountPath: /tmp
        livenessProbe:
          httpGet: { path: /livez, port: https, scheme: HTTPS }
          initialDelaySeconds: 20
          periodSeconds: 10
        readinessProbe:
          httpGet: { path: /readyz, port: https, scheme: HTTPS }
          initialDelaySeconds: 20
          periodSeconds: 10
        securityContext:
          readOnlyRootFilesystem: true
        resources:
          requests: { cpu: 50m, memory: 60Mi }
          limits:   { cpu: 200m, memory: 200Mi }
---
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: v1beta1.metrics.k8s.io
spec:
  group: metrics.k8s.io
  version: v1beta1
  insecureSkipTLSVerify: true
  groupPriorityMinimum: 100
  versionPriority: 100
  service:
    name: metrics-server
    namespace: kube-system
    port: 443

# =========================
# MONITORING: Grafana (own Deployment + LB on port 80) & Prometheus
# =========================
---
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
# Grafana Config (monitoring namespace)
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasource
  namespace: monitoring
  labels:
    grafana_datasource: "1"
data:
  prometheus.yaml: |
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus.monitoring.svc.cluster.local:9090
      isDefault: true
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  oke-autoscale.json: |
    {
      "title": "OKE Autoscale: Node CPU + Capacity",
      "timezone": "browser",
      "schemaVersion": 38,
      "version": 11,
      "refresh": "10s",
      "panels": [
        {
          "type": "timeseries",
          "title": "Node CPU Utilization (%) by node",
          "targets": [
            {
              "expr": "100 * sum by (node) (rate(container_cpu_usage_seconds_total{job=\"kubelet-cadvisor\",container!~\"^POD$|^$\",image!=\"\"}[2m])) / on (node) group_left sum by (node) (machine_cpu_cores{job=\"kubelet-cadvisor\"})",
              "legendFormat": "{{node}}"
            }
          ],
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 }
        },
        {
          "type": "timeseries",
          "title": "Cluster CPU Utilization (%)",
          "targets": [
            {
              "expr": "100 * sum(rate(container_cpu_usage_seconds_total{job=\"kubelet-cadvisor\",container!~\"^POD$|^$\",image!=\"\"}[2m])) / sum(machine_cpu_cores{job=\"kubelet-cadvisor\"})",
              "legendFormat": "cluster"
            }
          ],
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 }
        },

        {
          "type": "timeseries",
          "title": "Node Memory Utilization (%) by node",
          "targets": [
            {
              "expr": "100 * sum by (node) (container_memory_working_set_bytes{job=\"kubelet-cadvisor\",container!~\"^POD$|^$\",image!=\"\"}) / on (node) group_left sum by (node) (machine_memory_bytes{job=\"kubelet-cadvisor\"})",
              "legendFormat": "{{node}}"
            }
          ],
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 }
        },
        {
          "type": "timeseries",
          "title": "Cluster Memory Utilization (%)",
          "targets": [
            {
              "expr": "100 * sum(container_memory_working_set_bytes{job=\"kubelet-cadvisor\",container!~\"^POD$|^$\",image!=\"\"}) / sum(machine_memory_bytes{job=\"kubelet-cadvisor\"})",
              "legendFormat": "cluster"
            }
          ],
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 }
        },

        {
          "type": "timeseries",
          "title": "Ready nodes (count)",
          "targets": [
            { "expr": "sum( max by (node) (k8s_node_ready_gauge{job=\"autoscale-probe\"}) )", "legendFormat": "ready" }
          ],
          "gridPos": { "h": 8, "w": 24, "x": 0, "y": 16 }
        },

        {
          "type": "timeseries",
          "title": "Cluster CPU capacity vs allocatable (cores)",
          "targets": [
            { "expr": "sum( max by (node) (k8s_node_capacity_cpu_cores{job=\"autoscale-probe\"}) )", "legendFormat": "capacity" },
            { "expr": "sum( max by (node) (k8s_node_allocatable_cpu_cores{job=\"autoscale-probe\"}) )", "legendFormat": "alloc" }
          ],
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 24 }
        },
        {
          "type": "timeseries",
          "title": "Cluster Memory capacity vs allocatable (GiB)",
          "targets": [
            { "expr": "sum( max by (node) (k8s_node_capacity_memory_bytes{job=\"autoscale-probe\"}) ) / 1024 / 1024 / 1024", "legendFormat": "capacity" },
            { "expr": "sum( max by (node) (k8s_node_allocatable_memory_bytes{job=\"autoscale-probe\"}) ) / 1024 / 1024 / 1024", "legendFormat": "alloc" }
          ],
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 24 }
        },

        {
          "type": "state-timeline",
          "title": "Node Ready state (per-node)",
          "targets": [
            { "expr": "max by (node) (k8s_node_ready_gauge{job=\"autoscale-probe\"})", "legendFormat": "{{node}}" }
          ],
          "fieldConfig": {
            "defaults": {
              "mappings": [
                {
                  "type": "value",
                  "options": {
                    "0": { "text": "NotReady", "color": "red" },
                    "1": { "text": "Ready",    "color": "green" }
                  }
                }
              ]
            },
            "overrides": []
          },
          "options": {
            "legend": { "displayMode": "list", "placement": "bottom" }
          },
          "gridPos": { "h": 8, "w": 24, "x": 0, "y": 32 }
        }
      ]
    }

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dash-provider
  namespace: monitoring
data:
  dashboards.yaml: |
    apiVersion: 1
    providers:
    - name: 'default'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: false
      editable: true
      options:
        path: /var/lib/grafana/dashboards
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
  labels: { app: grafana }
spec:
  replicas: 1
  selector:
    matchLabels: { app: grafana }
  template:
    metadata:
      labels: { app: grafana }
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:10.4.5
        env:
        - { name: GF_SECURITY_ADMIN_USER, value: "admin" }
        - { name: GF_SECURITY_ADMIN_PASSWORD, value: "admin" }
        - { name: GF_PATHS_PROVISIONING, value: /etc/grafana/provisioning }
        ports:
        - name: http
          containerPort: 3000
        resources:
          requests:
            cpu: "1"
            memory: "1Gi"
        volumeMounts:
        - { name: datasource,      mountPath: /etc/grafana/provisioning/datasources }
        - { name: dashboards,      mountPath: /etc/grafana/provisioning/dashboards }
        - { name: dashboards-json, mountPath: /var/lib/grafana/dashboards }
      volumes:
      - name: datasource
        configMap: { name: grafana-datasource }
      - name: dashboards
        projected:
          sources:
          - configMap: { name: grafana-dash-provider }
      - name: dashboards-json
        configMap: { name: grafana-dashboard }
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
  labels: { app: grafana }
  annotations:
    oci.oraclecloud.com/load-balancer-type: "lb"
    oci.oraclecloud.com/load-balancer-internal: "false"
    oci.oraclecloud.com/load-balancer-shape: "flexible"
    oci.oraclecloud.com/load-balancer-shape-flex-min: "10"   # Mbps
    oci.oraclecloud.com/load-balancer-shape-flex-max: "10"   # Mbps
    oci-load-balancer.oraclecloud.com/health-check: '{"protocol":"TCP","port":3000,"retries":2,"timeoutInMillis":5000}'
    service.beta.kubernetes.io/oci-load-balancer-security-list-management-mode: "None" # Needed for Use Pod IPs
spec:
  type: LoadBalancer
  allocateLoadBalancerNodePorts: false # Use Pod IPs
  selector: { app: grafana }
  externalTrafficPolicy: Cluster
  ports:
  - name: http
    port: 80
    targetPort: http
---
# Prometheus (scrapes autoscale app + kubelet cAdvisor)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources: ["nodes","nodes/metrics","nodes/proxy","services","endpoints","pods","namespaces"]
  verbs: ["get","list","watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: monitoring
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    scrape_configs:
    # Scrape ONLY the autoscale-probe metrics Service
    - job_name: 'autoscale-probe'
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names: ['autoscale']
      relabel_configs:
      - { action: keep,  source_labels: [__meta_kubernetes_service_name],       regex: autoscale-probe-metrics }
      - { action: keep,  source_labels: [__meta_kubernetes_endpoint_port_name], regex: metrics }
      - { action: labelmap, regex: __meta_kubernetes_service_label_(.*) }

    # Scrape kubelet cAdvisor (via API server proxy)
    - job_name: 'kubelet-cadvisor'
      scheme: https
      tls_config:
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      # add a stable "node" label for Grafana groupings
      - source_labels: [__meta_kubernetes_node_name]
        target_label: node
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
  labels: { app: prometheus }
spec:
  replicas: 1
  selector:
    matchLabels: { app: prometheus }
  template:
    metadata:
      labels: { app: prometheus }
    spec:
      serviceAccountName: prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:v2.54.1
        args:
        - --config.file=/etc/prometheus/prometheus.yml
        - --storage.tsdb.path=/prometheus
        - --web.enable-lifecycle
        ports:
        - name: http
          containerPort: 9090
        resources:
          requests:
            cpu: "1"
            memory: "1Gi"
        volumeMounts:
        - { name: config, mountPath: /etc/prometheus }
        - { name: data,   mountPath: /prometheus }
      volumes:
      - name: config
        configMap: { name: prometheus-config }
      - name: data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
  labels: { app: prometheus }
spec:
  type: ClusterIP
  selector: { app: prometheus }
  ports:
  - name: http
    port: 9090
    targetPort: http

# =========================
# CLUSTER AUTOSCALER RBAC (as provided)
# =========================
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
rules:
- apiGroups: [""]
  resources: ["events", "endpoints", "pods/eviction", "pods/status"]
  verbs: ["create", "patch", "update", "get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods", "services", "replicationcontrollers", "persistentvolumeclaims", "persistentvolumes"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["replicasets", "statefulsets", "daemonsets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch"]
  resources: ["jobs", "cronjobs"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities", "volumeattachments"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch", "update"]
- apiGroups: [""]
  resources: ["nodes/status"]
  verbs: ["patch"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["get", "list", "watch", "create", "update", "delete"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["namespaces"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
- kind: ServiceAccount
  name: cluster-autoscaler
  namespace: kube-system